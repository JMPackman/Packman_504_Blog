---
title: "Exploratory Factor Analysis"
author: "James Packman"
date: "2025-04-27"
categories: [news, code, analysis]
image: "consensus_plot.png"
---

Let's get started with exploratory factor analysis (EFA). This is a method for doing factor analysis when you don't have an explicit hypothesis about the underlying factor structure (i.e., which variables go with which factors).

# Data Overview

First, let's discuss the example data set we'll be using. These data are taken from a subset of survey data collected by the ADL and National Opinion Research Center (NORC) on people's attitudes about Jewish people in the United States.

Specifically, the data we'll be using contain participants' responses to questions assessing their beliefs in stereotypes about Jewish people in the U.S. These items comprise the Scale of Antisemitic Stereotypes, a.k.a. the "SASS" (Packman et al., under review; will be linked here when/if published). Check out the methodological appendix here for information about the survey and data collection: <https://www.adl.org/resources/report/antisemitic-attitudes-america-topline-findings>

The scale, as it appeared in the survey, appeared like this:

Instructions: "Below are a number of statements with which you will agree or disagree. There are absolutely no right or wrong answers. Use the specified scale to indicate the number that best matches your response to each statement."

1.  Jewish people are often very rude.

2.  Jewish people tend to complain a lot.

3.  All things considered, Jewish people are untrustworthy.

4.  Jewish people can be sneaky.

5.  When they feel slighted, Jewish people will be vengeful.

6.  Jewish people tend to be very good with money.

7.  Jewish people tend to influence the media.

8.  When it comes to education, Jewish people tend to be overachievers.

9.  On the whole, Jewish people are loyal to Israel.

Participants respond to each stereotype statement on a Likert scale from 1 ("strongly disagree") to 6 ("Strongly agree"). An even-numbered Likert scale prevents participants from hiding behind neutral answers. Note that we treat responses to each item as a continuous variable. There is sufficient justification for doing so in theory and past work, which I describe in the manuscript I will eventually link here).

## Data Import & Cleaning

Let's begin by loading the required packages.

```{r}
#| output: false
#Load Required Packages
library(GPArotation)
library(tidyr)
library(nFactors)
library(tidyverse)
library(haven)
library(psych)
library(dplyr)
library(Rmisc)
library(coefficientalpha)
library(lavaan)
library(plyr)
library(ggpubr)
library(mvnormtest)
library(QuantPsyc)
library(ltm)
library(easystats)
library(performance)
library(parameters)
library(knitr)
library("papaja")
library(Rmisc)
library(InteractionPoweR)
library(magick)
```

Next, we'll upload and clean the data (please reach out if, for some reason, you don't have access to the data). Don't worry about the data cleaning. It mostly involves list-wise deletion of NAs because factor analysis is really bad with missing data. The questions we're interested in were only presented to N = 1960 out of 4176 participants, so we have a lot of blank space in the data set corresponding to participants we don't need to assess. Note that we only remove these NAs for factor analyses; for other analyses, we use other methods.

```{r}
#| echo: false
#Load in data set
ADLdata1 <- read_dta("ADL2022_Reweighted_9Dec22.dta")
SASSADL1 <- data.frame(ADLdata1[,149:157])
#Drop NAs
SASSNoNA <- na.omit(SASSADL1)
#Cleaning data by removing "98s" because that corresponds to refusal to answer 
SASSclean <- SASSNoNA[!(SASSNoNA$Q39==98 | SASSNoNA$Q40==98 |SASSNoNA$Q41==98 | SASSNoNA$Q42==98 |SASSNoNA$Q43==98 | SASSNoNA$Q44==98 |SASSNoNA$Q45==98 |SASSNoNA$Q46==98 |SASSNoNA$Q47==98),]
#Renaming variables
SASSclean2 <- SASSclean %>%
  mutate(Jewish.people.are.often.very.rude = SASSclean$Q39,
          Jewish.people.tend.to.complain.a.lot = Q40,
         All.things.considered.Jewish.people.are.untrustworthy = Q41,
         Jewish.people.can.be.sneaky = Q42,
         When.they.feel.slighted.Jewish.people.will.be.vengeful = Q43,
         Jewish.people.tend.to.be.good.with.money = Q44,
         Jewish.people.tend.to.influence.the.media = Q45,
         When.it.comes.to.education.Jewish.people.tend.to.be.overachievers = Q46,
         On.the.whole.Jewish.people.are.loyal.to.Israel = Q47
         )
#Clean dataframe
SASSEFA <- data.frame(SASSclean2[,10:18])
```

## Exploratory Data Analysis

Let's briefly take a look at the data. We'll calculate summary statistics and internal consistency.

```{r}
summary.data.frame(SASSEFA) %>%
  kable()

cronbach.alpha(SASSEFA, CI = TRUE)
```

Our Cronbach's alpha is pretty high: 0.832, 95% C:I \[0.817, 0.845\]. But let's make a nicer-looking table.

```{r}
#| echo: false
SASSEFA_table <- structure(
  list(
    Questions = c(
      "Jewish people are often very rude",
      "Jewish people tend to complain a lot",
      "All things considered, Jewish people are untrustworthy",
      "Jewish people can be sneaky",
      "When they feel slighted, Jewish people will be vengeful",
      "Jewish people tend to influence the media",
      "Jewish people tend to be good with money",
      "When it comes to education, Jewish people tend to be overachievers",
      "On the whole, Jewish people are loyal to Israel"
    ),
    `Mean` = c(mean(SASSEFA$Jewish.people.are.often.very.rude), mean(SASSEFA$Jewish.people.tend.to.complain.a.lot), mean(SASSEFA$All.things.considered.Jewish.people.are.untrustworthy), mean(SASSEFA$Jewish.people.can.be.sneaky), mean(SASSEFA$When.they.feel.slighted.Jewish.people.will.be.vengeful), mean(SASSEFA$Jewish.people.tend.to.influence.the.media), mean(SASSEFA$Jewish.people.tend.to.be.good.with.money), mean(SASSEFA$When.it.comes.to.education.Jewish.people.tend.to.be.overachievers), mean(SASSEFA$On.the.whole.Jewish.people.are.loyal.to.Israel)), 
    `Median` = c(median(SASSEFA$Jewish.people.are.often.very.rude), median(SASSEFA$Jewish.people.tend.to.complain.a.lot), median(SASSEFA$All.things.considered.Jewish.people.are.untrustworthy), median(SASSEFA$Jewish.people.can.be.sneaky), median(SASSEFA$When.they.feel.slighted.Jewish.people.will.be.vengeful), median(SASSEFA$Jewish.people.tend.to.influence.the.media), median(SASSEFA$Jewish.people.tend.to.be.good.with.money), median(SASSEFA$When.it.comes.to.education.Jewish.people.tend.to.be.overachievers), median(SASSEFA$On.the.whole.Jewish.people.are.loyal.to.Israel)),
    `SD` = c(sd(SASSEFA$Jewish.people.are.often.very.rude), sd(SASSEFA$Jewish.people.tend.to.complain.a.lot), sd(SASSEFA$All.things.considered.Jewish.people.are.untrustworthy), sd(SASSEFA$Jewish.people.can.be.sneaky), sd(SASSEFA$When.they.feel.slighted.Jewish.people.will.be.vengeful), sd(SASSEFA$Jewish.people.tend.to.influence.the.media), sd(SASSEFA$Jewish.people.tend.to.be.good.with.money), sd(SASSEFA$When.it.comes.to.education.Jewish.people.tend.to.be.overachievers), sd(SASSEFA$On.the.whole.Jewish.people.are.loyal.to.Israel))
  ),
  class = "data.frame",
  row.names = c(NA, 9L))
 # SASSEFA_table[NA, -1] <- apa_num(SASSEFA_table[NA, -1])

  apa_table(
  SASSEFA_table,
  caption = "Scale of Antisemitic Stereotypes Descriptive Statistics",
  col_spanners = list("Score" = c(1,4)),
  note = "Item scores are scored 1-6 ('strongly disagree' - strongly agree')",
  landscape = FALSE
)
```

# Pre-EFA Tests

Next, we have to make sure that our data is suitable for factor analysis. We need to conduct two tests:

## Kaiser-Meyer-Olkin Test

The Kaiser-Meyer-Olkin test (KMO) calculates a measure of sampling adequacy (MSA) for each item, and for your items overall. MSA less than 0.50 is really bad.

```{r}
#Is factor analysis is warranted?
KMO(SASSEFA) # You should get rid of all variables with MSA < 0.50, which we don't need to do here. 
```

Overall KMO score is 0.88, which is quite good (above 0.90 is ideal, but 0.88 is pretty good).

## Bartlett's Test of Sphericity

Next, we do Bartlett's test of sphericity, which is a linear algebra/matrix-related test we're not going to get into here. Suffice it to say that Bartlett's test evaluates whether the variables in our data are sufficiently correlated to proceed with factor analysis.

```{r}
cortest.bartlett(SASSEFA)
```

Fun fact, we can use the performance package to conduct both of these tests at once! The output even provides an explanatory example write-up!

```{r}
performance::check_factorstructure(SASSEFA)
```

# Number of Factors

Next, we have to determine how many factors we should extract from our data. There are several methods to do this.

## Kaiser Criterion (Eigenvalues)

According to Kaiser, if a factor has an Eigenvalue of 1 or higher, that factor should be retained.

"Eigenvalue" is another linear algebra term. It essentially tells you how data are spread out on a "line," or in this case, an Eigenvector. Eigenvalues are scalar transformations applied to Eigenvectors, which themselves have magnitudes and directions (this will be important when we talk about rotation in a minute).

```{r}
ev <- eigen(cor(SASSEFA)) # get Eigenvalues
ev$values
```

According to the output of the above code, the first two factors out of a potential 9 factors (i.e., one factor per variable, which would defeat the purpose of having factors) have Eigenvalues above one. This would suggest we retain two factors.

We can also get Eigenvalues from Scree plots, like the ones in the code chunk below. The second plot overlays parallel analysis, which used actual, simulated, and re-sampled data.

```{r}
scree(SASSEFA, pc=FALSE)  # Use pc=FALSE for factor analysis. "PC" refers to principal components analysis, which is beyond the scope of this tutorial (though it is related).
fa.parallel(SASSEFA, fa="fa") #FA for Factor Analysis.

```

In Scree plots, we're looking for the "elbow" of the plot; that is, the inflection point after which each factor you add has Eigenvalues below 1. As the output helpfully tells us, this would suggest 2 factors.

## Consensus Method

While Kaiser is arguable the most common method for determining the number of factors, there's a neat function called n_factors which compares across multiple criteria for factor inclusion. Basically, it surveys multiple methods of choosing the number of factors you want, then gives you a sort of histogram to tell you how many methods encourage using each possible number of factors.

```{r}
#Factor analysis is warranted. We must identify the appropriate number of factors. We set n_max at 9 because that's the largest number of factors we could have in this 9-item data set. 
n_factors(SASSEFA, n_max = 9) %>%
  plot()
```

According to this plot, 2 factors seems to be the consensus among different methods. The dashed line represents the cumulative variance in the data explained by each number of factors you could extract. For 2 factors, we're looking at just above 50% variance explained. Ideally, for factor analysis, we would prefer to see something closer to 70%.

# Factor Loadings

Now that we've settled on picking 2 factors, we have to figure out how well each variable "maps onto" each factor, which is called the "Factor loadings." To do that, we need to choose what kind of rotation and what kind of factoring method (estimator) we want to use. Importantly, a variable can load onto multiple different factors at once, which is called "cross-loading." Different rotation methods deal with cross-loading in different ways.

## Rotation

"Rotation" has to do with how different the directions of vectors are. In our terms, it refers to how correlated or uncorrelated we think our factors should be.

If we assume our factors are entirely uncorrelated (i.e., independent), then we would use an orthogonal rotation to find our factor loadings.

### Orthogonal Rotations

#### Varimax Rotation

Varimax rotation tries to reduce cross-loading and minimizes smaller loadings. This gives you clear, unambiguous factor structures.

#### Quartimax Rotation

Quartimax rotation tries to reduce the number of variables you need to comprise each factor. This makes interpretation simpler, but potentially at loss of some complexity.

#### Equamax Rotation

This is just a compromise between varimax and quartimax.

As an aside, orthogonality in 2D geometry is perpendicularity. So to understand orthogonality, think about two lines separated by a 90-degree angle, like an X- and Y-axis. No matter how far you travel along one axis, you don't move at all along the other axis. These two axes, also called dimensions are thus independent, or not correlated. As we discussed, our factors are, in theory terms, a dimension. Furthermore, our factors can be expressed mathematically as vectors. Therefore, assuming an orthogonal factor structure, which orthogonal rotations do, would be to assume that one factor is uncorrelated with the other. If we drew the factors/vectors in 2D space, they would be perpendicular.

### Oblique Rotation

Assuming orthogonality can be somewhat restrictive. It's quite possible, depending on your theoretical question, that your factors will be correlated in some way. To capture this correlation, we can use oblique rotations.

#### Promax Rotation

Promax rotation is really good at structuring large data sets. However, it might lead to higher correlations between factors.

#### Oblimin Rotation

This approach can't handle large data sets as well as promax; however, it often produces a simple, easier-to-interpret factor structure.

Which rotation method you choose depends a lot on your prior knowledge of the phenomena you're studying. In our case, we are interested in stereotypes, which tend to be correlated. As such, we'll use an oblique rotation. Past work has used oblimin rotation, so we will use that method here.

## Factoring Method

We also have to specify our factoring method. Type "?fa" into R to find the documentation and a brief rationale for each "fm" (factoring method). The two best suited to our analysis here are fm="pa," which gives the *p*rincipal factor solution, and fm="ml," which gives the *m*aximum *l*ikelihood factor analysis.

Note how we include the rotation method and factoring method in the code below

```{r}
#Oblimin Rotation; principal factor solution
SASS_efa_pa <- psych::fa(SASSEFA, nfactors = 2, rotate="oblimin", fm="pa") %>% model_parameters(sort = TRUE, threshold = "max")

SASS_efa_pa

#Oblimin Rotation; maximum likelihood solution

SASS_efa_ML <- psych::fa(SASSEFA, nfactors = 2, rotate="oblimin", fm="ml") %>% model_parameters(sort = TRUE, threshold = "max")

SASS_efa_ML
```

Columns PA1 and PA2 give you the factor loadings for each factor in the principal factor solution, while columns ML1 and ML2 give factor loadings for each variable in the maximum likelihood solution. The "threshold = MAX" argument hides cross-loadings for each variable on the other factor (i.e., the factor onto which they load less strongly).

Although the two solutions (PA and ML) reveal virtually no difference with out data, it's good to be familiar with multiple factoring methods in EFA.

From the above output, we can see that 6 of our variables load onto the first factor, which explains roughly 38% of the variance. 3 variables load onto the second factor, which explains just over 14% of the variance. Cumulatively, our factors explain roughly 52% of the variance in our data (which is what we saw in the consensus methods plot above). 52% isn't super high, but it's a good start.

Looking at uniqueness, which refers to the proportion of variance for each variable that the factors explain, we see that the items on factor 2 tend to have higher uniqueness, meaning their variance is less well explained by our factors. The last item is particularly unique, despite it loading best onto factor 2.

# Interpreting Factors

We now have an understanding of which variables map onto which factors. Let's calculate internal consistency.

```{r}
Factor1 <- SASSEFA %>%
    dplyr::select("Jewish.people.are.often.very.rude",
                  "Jewish.people.tend.to.complain.a.lot",
                  "All.things.considered.Jewish.people.are.untrustworthy",
                    "Jewish.people.can.be.sneaky",
                    "When.they.feel.slighted.Jewish.people.will.be.vengeful",
                    "Jewish.people.tend.to.influence.the.media")
Factor2 <- SASSEFA %>%
  dplyr::select("Jewish.people.tend.to.be.good.with.money",
         "When.it.comes.to.education.Jewish.people.tend.to.be.overachievers",
         "On.the.whole.Jewish.people.are.loyal.to.Israel")
cronbach.alpha(Factor1, standardized = TRUE, CI = TRUE)
cronbach.alpha(Factor2, standardized = TRUE, CI = TRUE)
```

Okay, Cronbach's alpha for factor one is very good: 0.887! Cronbach's alpha for our second factor, however, is a bit weaker at 0.635. So, what we may have here is one really clear factor (factor 1) plus some "extra stuff" that loosely goes together.

What we name these two factors (i.e., how we refer to them) really depends on preexisting theory. According to the stereotype content model (SCM), stereotype perceptions tend to fall along two dimensions; warmth (i.e., friendliness, morality), and competence (i.e., agency, ability; Fiske et al., 2007; Fiske, 2018). Factor 1 comprises stereotypes about Jewish people being antisocial: there are stereotypes about their tendencies to be rude, to complain, to seek revenge, and to deceive others. We can thus call factor 1 a "warmth" dimension. Factor 2 includes stereotypes about education, money, and global conspiracy (i.e., loyalty to a foreign power). so we'll tentatively call this a "competence" dimension.

Now that we've done EFA, we'll use another data set to test our hypothesis that these antisemitic stereotypes have an underlying 2-factor structure. See you in the next post, which will be on Confirmatory Factor Analysis!
