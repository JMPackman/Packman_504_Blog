---
title: "Confirmatory Factor Analysis"
author: "James Packman"
date: "2025-05-01"
categories: [news, code, analysis]
image: "confirm copy.png"
---

Now that you have a solid foundation for factor analysis, and have completed EFA, let's turn our attention to confirmatory factor analysis (CFA). You're workflow should consist of EFA, then CFA. CFA is the factor analysis you conduct when you have a hypothesis about what the underlying factor structure among variables in your data should look like.

Note that you shouldn't conduct EFA and CFA on the same data set. That would be like betting on a baseball game you've already seen. Instead, you should watch some baseball games (your EFA), then, when you have a sense of the pattern, bet on the next one (note: this post does not constitute an endorsement of gambling).

Consequently, we got a whole new data set for a whole new ball game. Note that scale responses range from 0-5, rather than 1-6 in this study (both are still 6-point Likert scales).

# Exploratory Data Analysis

Let's load our required packages and then our data set, which has the same variables from last time.

```{r}
#| output: false
library("papaja")
library(GPArotation)
library(tidyr)
library(nFactors)
library(tidyverse)
library(haven)
library(psych)
library(dplyr)
library(Rmisc)
library(coefficientalpha)
library(lavaan)
library(ggpubr)
library(mvnormtest)
library(QuantPsyc)
library(ltm)
library(easystats)
library(performance)
library(parameters)
library(InteractionPoweR)
library(magick)
library(plyr)
library(knitr)
```

```{r}
#Load in data set
SASSCFAdata <- read.csv("SASSCFA.csv")  

#Subset columns to only include Scale of Antisemitic Stereotypes (SASS)
SASSCFA <- data.frame(SASSCFAdata[,44:52])
#Summary Statistics - Full Scale

cronbach.alpha(SASSCFA, CI = TRUE)
```

We get a similar Cronbach's alpha as we did last time; we've got pretty good internal consistency for the full set of variables. Here's another summary table.

```{r}
#| echo: false
SASSCFA_table <- structure(
  list(
    Questions = c(
      "Jewish people are often very rude",
      "Jewish people tend to complain a lot",
      "All things considered, Jewish people are untrustworthy",
      "Jewish people can be sneaky",
      "When they feel slighted, Jewish people will be vengeful",
      "Jewish people tend to influence the media",
      "Jewish people tend to be good with money",
      "When it comes to education, Jewish people tend to be overachievers",
      "On the whole, Jewish people are loyal to Israel"
    ),
    `Mean` = c(mean(SASSCFA$Jewish.people.are.often.very.rude.), mean(SASSCFA$Jewish.people.tend.to.complain.a.lot.), mean(SASSCFA$All.things.considered..Jewish.people.are.untrustworthy.), mean(SASSCFA$Jewish.people.can.be.sneaky.), mean(SASSCFA$When.they.feel.slighted..Jewish.people.will.be.vengeful.), mean(SASSCFA$Jewish.people.tend.to.influence.the.media.), mean(SASSCFA$Jewish.people.tend.to.be.good.with.money.), mean(SASSCFA$When.it.comes.to.education..Jewish.people.tend.to.be.overachievers.), mean(SASSCFA$On.the.whole..Jewish.people.are.loyal.to.Israel.)),
    `Median` = c(median(SASSCFA$Jewish.people.are.often.very.rude.), median(SASSCFA$Jewish.people.tend.to.complain.a.lot.), median(SASSCFA$All.things.considered..Jewish.people.are.untrustworthy.), median(SASSCFA$Jewish.people.can.be.sneaky.), median(SASSCFA$When.they.feel.slighted..Jewish.people.will.be.vengeful.), median(SASSCFA$Jewish.people.tend.to.influence.the.media.), median(SASSCFA$Jewish.people.tend.to.be.good.with.money.), median(SASSCFA$When.it.comes.to.education..Jewish.people.tend.to.be.overachievers.), median(SASSCFA$On.the.whole..Jewish.people.are.loyal.to.Israel.)),
    `SD` = c(sd(SASSCFA$Jewish.people.are.often.very.rude.), sd(SASSCFA$Jewish.people.tend.to.complain.a.lot.), sd(SASSCFA$All.things.considered..Jewish.people.are.untrustworthy.), sd(SASSCFA$Jewish.people.can.be.sneaky.), sd(SASSCFA$When.they.feel.slighted..Jewish.people.will.be.vengeful.), sd(SASSCFA$Jewish.people.tend.to.influence.the.media.), sd(SASSCFA$Jewish.people.tend.to.be.good.with.money.), sd(SASSCFA$When.it.comes.to.education..Jewish.people.tend.to.be.overachievers.), sd(SASSCFA$On.the.whole..Jewish.people.are.loyal.to.Israel.))
  ),
  class = "data.frame",
  row.names = c(NA, 9L))
  SASSCFA_table[, -1] <- apa_num(SASSCFA_table[, -1])

  apa_table(
  SASSCFA_table,
  caption = "Scale of Antisemitic Stereotypes Descriptive Statistics - CFA Study",
  col_spanners = list("Score" = c(1,4)),
  note = "Item scores are scored 0-5 ('strongly disagree' - strongly agree')",
  landscape = FALSE
)
```

# Pre-CFA Test

Next, we have to make sure our data are suitable for CFA.

## Multivariate Normality Test

Factor analysis assumes multivariate normality; that is, that the data are roughly normally distributed in multivariate space. We can also use the performance::check we did in EFA to conduct the KMO and Sphericity tests.

```{r}
#Mardia's Multivariate Normality Test
mult.norm(SASSCFA)$mult.test

performance::check_factorstructure(SASSCFA)
```

Unfortunately, because the test returned significant results, that means our data are not only skewed, but also leptokurtic. There is some evidence to suggest that this is rather common in socially sensitive research areas like stereotyping and prejudice. We'll do our best to account for these characteristics of the data going forward.

But the good news is, the other tests indicated that our data has adequate sampling and correlation for factor analysis. Onward!

# Model Specification and Fit

We next specify which variables we think go into which factors. This is where our previous EFA is helpful; we can organize the variables into the factors onto which they loaded in EFA. We name the factors "warmth" and "competence" based on our prior understanding of the theory.

```{r}
#Two-factor (TF) model specification
TF.model <- ' warmth =~ Jewish.people.are.often.very.rude. +
                  Jewish.people.tend.to.complain.a.lot. +
                  All.things.considered..Jewish.people.are.untrustworthy. +
                    Jewish.people.can.be.sneaky. +
                    When.they.feel.slighted..Jewish.people.will.be.vengeful. +
                    Jewish.people.tend.to.influence.the.media. 
              competence =~  Jewish.people.tend.to.be.good.with.money. +
         When.it.comes.to.education..Jewish.people.tend.to.be.overachievers. +
         On.the.whole..Jewish.people.are.loyal.to.Israel.'
```

Next, we'll calculate how well this two-factor model fits our data. Because the data is skewed and leptokurtic, we'll use a maximum likelihood estimation with robust (Huber-White) standard errors.

```{r}
fitTF <-cfa(TF.model, data = SASSCFA, estimator="MLR") #Maximum likelihood estimator
summary(fitTF, fit.measures = TRUE)
```

Okay, this output gives us a lot. Note the covariance between the factors! But we can make our lives easier by computing the model fit indices we want:

```{r}
#We use "scaled" to provide a scaling correction factor (i.e., the Yuan-Bentler correction, Mplus variant)
fitTF.scaled<-c("chisq.scaled", "cfi.scaled", "tli.scaled", "rmsea.scaled", "srmr", "aic", "bic")  
Two_Factor_Fit <- fitmeasures(fitTF, fitTF.scaled)
Two_Factor_Fit %>%
  kable()
```

This is much easier to interpret. But what are we comparing this two-factor model against? We also need to specify our one-factor "baseline" model. This is the model where all the variables are under the same factor (i.e., there are no underlying factors; everything goes together.

```{r}
#One-factor (OF) model specification and fit
OF.model <- ' all =~ Jewish.people.are.often.very.rude. +
                  Jewish.people.tend.to.complain.a.lot. +
                  All.things.considered..Jewish.people.are.untrustworthy. +
                    Jewish.people.can.be.sneaky. +
                    When.they.feel.slighted..Jewish.people.will.be.vengeful. +
                    Jewish.people.tend.to.influence.the.media. + Jewish.people.tend.to.be.good.with.money. +
         When.it.comes.to.education..Jewish.people.tend.to.be.overachievers. +
         On.the.whole..Jewish.people.are.loyal.to.Israel. '
fitOF <-cfa(OF.model, data = SASSCFA, estimator="MLR")
summary(fitOF, fit.measures = TRUE)
fitOF.scaled<-c("chisq.scaled", "cfi.scaled", "tli.scaled", "rmsea.scaled", "srmr", "aic", "bic")  
One_Factor_Fit <- fitmeasures(fitOF, fitOF.scaled) 
One_Factor_Fit %>%
  kable()
```

## Model Comparison

We can now compare the fit indices we calculated for our two-factor and one-factor models. To make this easier, let's put them in a table. We'll also add in the "ideal" cutoff criteria from Hu & Bentler (1999):

```{r}
#| echo: false
SASS_CFAfit_table <- structure(
  list(
    Fit_Index = c(
      "Chi-Sq. Scaled",
      "CFI Scaled",
      "TLI Scaled",
      "RMSEA Scaled",
      "SRMR",
      "AIC",
      "BIC"
    ),
    `One Factor Model` = One_Factor_Fit,
    `Two Factor Model` = Two_Factor_Fit,
    `Hu & Bentler (1999) Cutoff Criteria` = c("-", "0.95", "0.95", "0.06", "0.08", "-", "-")
  ),
  class = "data.frame",
  row.names = c(NA, 7L))
  SASS_CFAfit_table[, -1] <- apa_num(SASS_CFAfit_table[, -1])

  apa_table(
  SASS_CFAfit_table,
  caption = "Scale of Antisemitic Stereotypes Factor Fit Indices",
  col_spanners = list("Score" = c(1,4)),
  note = "Maximum Likelihood estimator with robust standard errors",
  landscape = FALSE
)
```

Based on these criteria, the two-factor model seems to fit the data better. The two-factor model also does a pretty good job of fulfilling the Hu & Bentler (1999) criteria, especially for such our relatively small sample (N = 394).

If you're still not convinced, you can also run an ANOVA comparing the two models:

```{r}
anova(fitOF, fitTF)
```

According to the output, the fit indices for our two-factor model are significantly different (i.e., better) than for our one-factor model.

## Interpreting our Factors

Now we return to the familiar process of unpacking and interpreting our factors. Let's get the factor loadings for our two-factor model:

```{r}
#Factor Loadings for 2-Factor Model
SASS_CFA_Loadings <- psych::fa(SASSCFA, nfactors = 2, rotate="oblimin", fm="ml") %>% model_parameters(sort = TRUE, threshold = "max")

SASS_CFA_Loadings
```

As you can see, our results are comparable to the loadings our EFA yielded. That gives us confidence that the factor structure of our data is consistent across samples! Once again, ML1 is our "warmth" dimension, while ML2 is our "competence" dimension.

Let's calculate the internal consistency of each factor.

```{r}
Warmth <- SASSCFA %>%
    dplyr::select("Jewish.people.are.often.very.rude.",
                  "Jewish.people.tend.to.complain.a.lot.",
                  "All.things.considered..Jewish.people.are.untrustworthy.",
                    "Jewish.people.can.be.sneaky.",
                    "When.they.feel.slighted..Jewish.people.will.be.vengeful.",
                    "Jewish.people.tend.to.influence.the.media.")
Competence <- SASSCFA %>%
  dplyr::select("Jewish.people.tend.to.be.good.with.money.",
         "When.it.comes.to.education..Jewish.people.tend.to.be.overachievers.",
         "On.the.whole..Jewish.people.are.loyal.to.Israel.")
WarmthAlpha <- cronbach.alpha(Warmth, standardized = TRUE, CI = TRUE)
CompAlpha <- cronbach.alpha(Competence, standardized = TRUE, CI = TRUE)
WarmthAlpha
CompAlpha

```

Once again, we have really good internal consistency with our first factor, but a little shaky with our second factor. So, it might be that the first factor is pretty solid, and the other items represent loosely-grouped "other stuff" outside of that factor.
