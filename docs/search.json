[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog:\nThis is my blog. There are many like it but this one is mine. Without me, my blog is useless. Well, I hope it isn’t useless. In fact, I hope it’s a useful tool to help introduce you to factor analysis.\nImage: The blogger, hard at work in his study."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to Factor Analysis",
    "section": "",
    "text": "Welcome to this tutorial blog on exploratory factor analysis (EFA) and confirmatory factor analysis (CFA)."
  },
  {
    "objectID": "posts/welcome/index.html#a-zoo-of-factors-an-example",
    "href": "posts/welcome/index.html#a-zoo-of-factors-an-example",
    "title": "Welcome to Factor Analysis",
    "section": "A Zoo of Factors: An Example",
    "text": "A Zoo of Factors: An Example\nImagine you’ve asked a bunch of people how much they like cats, dogs, hamsters, flies, spiders, beetles, lions, tigers, and bears (oh my!). People’s opinions about cats, dogs, and hamsters probably cluster together, as do their views of lions, tigers, and bears. Based on people’s responses, you could conceivably group together their views on “Pets” (cats, dogs, hamsters), “Bugs” (flies, spiders, beetles), and “Predators” (lions, tigers, bears). People’s opinions about “Pets,” “Bugs,” and “Predators” are your factors, while their responses about each type of animal are your variables. But it’s much easier to look at patterns between factors than variables. For instance, maybe the more people like predators, the more they like bugs. I don’t know why that would be the case, but suffice it to say that grouping variables into factors allows you to ask higher-level questions."
  },
  {
    "objectID": "posts/welcome/index.html#building-theory-out-of-higher-level-constructs",
    "href": "posts/welcome/index.html#building-theory-out-of-higher-level-constructs",
    "title": "Welcome to Factor Analysis",
    "section": "Building Theory out of Higher-Level Constructs",
    "text": "Building Theory out of Higher-Level Constructs\nFactors not only make your data less cumbersome, but also can help you answer questions about theoretical constructs. If you’re interested in an esoteric concept, like “well-being,” that can be difficult to measure directly in an accurate, meaningful way. You probably wouldn’t just want to ask “how would you rate your well-being on a scale of 1-10?” You could do that, but you could be a lot more confident in your results if you instead measured a bunch of different things that comprise well-being.\nYou might have different theories about what goes into well-being. Perhaps physical health and social connection are two “buckets” of interest (the metaphor returns!). You could measure variables related to physical health (e.g., diet, exercise) and social connection (e.g., number of close friends, frequency of interpersonal contact). If each of those variables reliably “go together” under their respective factors, you have evidence that those variables collectively provide you a good sense of that factor. “Going together” indicates internal consistency (e.g., Cronbach’s alpha) as well as accounting for variance in your data. But we’ll get into this more later.\nIn our example, if the physical health variables go together, your exercise and diet variables probably give you a good picture of this higher-level concept of “physical health.” Furthermore, if the factors—physical health and social connection, collectively explain a lot of the variance in your data, that might suggest you have a richer understanding of what goes into “well-being” than you would get if you just asked people to report their well-being on a single-item scale.\nThis thought process is what undergirds factor analysis. It’s also one of the rationales of scale construction: if you want a coherent scale that measures a clear construct, you need factor analysis.\nReducing dimensionality, clarifying theoretical constructs, and formulating measurement scales are just some of the things you can do with factor analysis. In the next posts, we’ll talk about exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) with example code.\nImage Credit: https://www.growthmentor.com/glossary/factor-analysis/"
  },
  {
    "objectID": "posts/post-with-code/CFA.html",
    "href": "posts/post-with-code/CFA.html",
    "title": "Confirmatory Factor Analysis",
    "section": "",
    "text": "Now that you have a solid foundation for factor analysis, and have completed EFA, let’s turn our attention to confirmatory factor analysis (CFA). You’re workflow should consist of EFA, then CFA. CFA is the factor analysis you conduct when you have a hypothesis about what the underlying factor structure among variables in your data should look like.\nNote that you shouldn’t conduct EFA and CFA on the same data set. That would be like betting on a baseball game you’ve already seen. Instead, you should watch some baseball games (your EFA), then, when you have a sense of the pattern, bet on the next one (note: this post does not constitute an endorsement of gambling).\nConsequently, we got a whole new data set for a whole new ball game. Note that scale responses range from 0-5, rather than 1-6 in this study (both are still 6-point Likert scales)."
  },
  {
    "objectID": "posts/post-with-code/CFA.html#multivariate-normality-test",
    "href": "posts/post-with-code/CFA.html#multivariate-normality-test",
    "title": "Confirmatory Factor Analysis",
    "section": "Multivariate Normality Test",
    "text": "Multivariate Normality Test\nFactor analysis assumes multivariate normality; that is, that the data are roughly normally distributed in multivariate space. We can also use the performance::check we did in EFA to conduct the KMO and Sphericity tests.\n\n#Mardia's Multivariate Normality Test\nmult.norm(SASSCFA)$mult.test\n\n         Beta-hat    kappa p-val\nSkewness  10.9399 718.3868     0\nKurtosis 120.9176  15.4589     0\n\nperformance::check_factorstructure(SASSCFA)\n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(36) = 1578.54, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.89). The individual KMO scores are: Jewish.people.tend.to.complain.a.lot. (0.92), Jewish.people.are.often.very.rude. (0.89), All.things.considered..Jewish.people.are.untrustworthy. (0.89), Jewish.people.can.be.sneaky. (0.91), When.they.feel.slighted..Jewish.people.will.be.vengeful. (0.91), Jewish.people.tend.to.influence.the.media. (0.93), Jewish.people.tend.to.be.good.with.money. (0.75), When.it.comes.to.education..Jewish.people.tend.to.be.overachievers. (0.84), On.the.whole..Jewish.people.are.loyal.to.Israel. (0.77).\n\n\nUnfortunately, because the test returned significant results, that means our data are not only skewed, but also leptokurtic. There is some evidence to suggest that this is rather common in socially sensitive research areas like stereotyping and prejudice. We’ll do our best to account for these characteristics of the data going forward.\nBut the good news is, the other tests indicated that our data has adequate sampling and correlation for factor analysis. Onward!"
  },
  {
    "objectID": "posts/post-with-code/CFA.html#model-comparison",
    "href": "posts/post-with-code/CFA.html#model-comparison",
    "title": "Confirmatory Factor Analysis",
    "section": "Model Comparison",
    "text": "Model Comparison\nWe can now compare the fit indices we calculated for our two-factor and one-factor models. To make this easier, let’s put them in a table. We’ll also add in the “ideal” cutoff criteria from Hu & Bentler (1999):\n\n\n\n(#tab:unnamed-chunk-9) Scale of Antisemitic Stereotypes Factor Fit Indices\n\n\nFit_Index\nOne Factor Model\nTwo Factor Model\nHu & Bentler (1999) Cutoff Criteria\n\n\n\n\nChi-Sq. Scaled\n175.46\n76.57\n-\n\n\nCFI Scaled\n0.88\n0.96\n0.95\n\n\nTLI Scaled\n0.84\n0.94\n0.95\n\n\nRMSEA Scaled\n0.12\n0.07\n0.06\n\n\nSRMR\n0.09\n0.06\n0.08\n\n\nAIC\n9,712.72\n9,596.71\n-\n\n\nBIC\n9,784.30\n9,672.26\n-\n\n\n\n\nNote. Maximum Likelihood estimator with robust standard errors\n\n \n\n\nBased on these criteria, the two-factor model seems to fit the data better. The two-factor model also does a pretty good job of fulfilling the Hu & Bentler (1999) criteria, especially for such our relatively small sample (N = 394).\nIf you’re still not convinced, you can also run an ANOVA comparing the two models:\n\nanova(fitOF, fitTF)\n\n\nScaled Chi-Squared Difference Test (method = \"satorra.bentler.2001\")\n\nlavaan-&gt;lavTestLRT():  \n   lavaan NOTE: The \"Chisq\" column contains standard test statistics, not the \n   robust test that should be reported per model. A robust difference test is \n   a function of two standard (not robust) statistics.\n      Df    AIC    BIC   Chisq Chisq diff Df diff Pr(&gt;Chisq)    \nfitTF 26 9596.7 9672.3  89.075                                  \nfitOF 27 9712.7 9784.3 207.086     72.864       1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAccording to the output, the fit indices for our two-factor model are significantly different (i.e., better) than for our one-factor model."
  },
  {
    "objectID": "posts/post-with-code/CFA.html#interpreting-our-factors",
    "href": "posts/post-with-code/CFA.html#interpreting-our-factors",
    "title": "Confirmatory Factor Analysis",
    "section": "Interpreting our Factors",
    "text": "Interpreting our Factors\nNow we return to the familiar process of unpacking and interpreting our factors. Let’s get the factor loadings for our two-factor model:\n\n#Factor Loadings for 2-Factor Model\nSASS_CFA_Loadings &lt;- psych::fa(SASSCFA, nfactors = 2, rotate=\"oblimin\", fm=\"ml\") %&gt;% model_parameters(sort = TRUE, threshold = \"max\")\n\nSASS_CFA_Loadings\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable                                                            |  ML1\n--------------------------------------------------------------------------\nAll.things.considered..Jewish.people.are.untrustworthy.             | 0.87\nJewish.people.are.often.very.rude.                                  | 0.85\nWhen.they.feel.slighted..Jewish.people.will.be.vengeful.            | 0.78\nJewish.people.can.be.sneaky.                                        | 0.77\nJewish.people.tend.to.complain.a.lot.                               | 0.76\nJewish.people.tend.to.influence.the.media.                          | 0.47\nJewish.people.tend.to.be.good.with.money.                           |     \nWhen.it.comes.to.education..Jewish.people.tend.to.be.overachievers. |     \nOn.the.whole..Jewish.people.are.loyal.to.Israel.                    |     \n\nVariable                                                            |  ML2\n--------------------------------------------------------------------------\nAll.things.considered..Jewish.people.are.untrustworthy.             |     \nJewish.people.are.often.very.rude.                                  |     \nWhen.they.feel.slighted..Jewish.people.will.be.vengeful.            |     \nJewish.people.can.be.sneaky.                                        |     \nJewish.people.tend.to.complain.a.lot.                               |     \nJewish.people.tend.to.influence.the.media.                          |     \nJewish.people.tend.to.be.good.with.money.                           | 0.74\nWhen.it.comes.to.education..Jewish.people.tend.to.be.overachievers. | 0.55\nOn.the.whole..Jewish.people.are.loyal.to.Israel.                    | 0.55\n\nVariable                                                           \n-------------------------------------------------------------------\nAll.things.considered..Jewish.people.are.untrustworthy.            \nJewish.people.are.often.very.rude.                                 \nWhen.they.feel.slighted..Jewish.people.will.be.vengeful.           \nJewish.people.can.be.sneaky.                                       \nJewish.people.tend.to.complain.a.lot.                              \nJewish.people.tend.to.influence.the.media.                         \nJewish.people.tend.to.be.good.with.money.                          \nWhen.it.comes.to.education..Jewish.people.tend.to.be.overachievers.\nOn.the.whole..Jewish.people.are.loyal.to.Israel.                   \n\nComplexity | Uniqueness\n-----------------------\n      1.02 |       0.30\n      1.02 |       0.33\n      1.03 |       0.33\n      1.01 |       0.37\n      1.01 |       0.40\n      1.76 |       0.55\n      1.01 |       0.47\n      1.05 |       0.65\n      1.00 |       0.69\n\nThe 2 latent factors (oblimin rotation) accounted for 54.51% of the total variance of the original data (ML1 = 39.42%, ML2 = 15.09%).\n\n\nAs you can see, our results are comparable to the loadings our EFA yielded. That gives us confidence that the factor structure of our data is consistent across samples! Once again, ML1 is our “warmth” dimension, while ML2 is our “competence” dimension.\nLet’s calculate the internal consistency of each factor.\n\nWarmth &lt;- SASSCFA %&gt;%\n    dplyr::select(\"Jewish.people.are.often.very.rude.\",\n                  \"Jewish.people.tend.to.complain.a.lot.\",\n                  \"All.things.considered..Jewish.people.are.untrustworthy.\",\n                    \"Jewish.people.can.be.sneaky.\",\n                    \"When.they.feel.slighted..Jewish.people.will.be.vengeful.\",\n                    \"Jewish.people.tend.to.influence.the.media.\")\nCompetence &lt;- SASSCFA %&gt;%\n  dplyr::select(\"Jewish.people.tend.to.be.good.with.money.\",\n         \"When.it.comes.to.education..Jewish.people.tend.to.be.overachievers.\",\n         \"On.the.whole..Jewish.people.are.loyal.to.Israel.\")\nWarmthAlpha &lt;- cronbach.alpha(Warmth, standardized = TRUE, CI = TRUE)\nCompAlpha &lt;- cronbach.alpha(Competence, standardized = TRUE, CI = TRUE)\nWarmthAlpha\n\n\nStandardized Cronbach's alpha for the 'Warmth' data-set\n\nItems: 6\nSample units: 394\nalpha: 0.897\n\nBootstrap 95% CI based on 1000 samples\n 2.5% 97.5% \n0.877 0.913 \n\nCompAlpha\n\n\nStandardized Cronbach's alpha for the 'Competence' data-set\n\nItems: 3\nSample units: 394\nalpha: 0.649\n\nBootstrap 95% CI based on 1000 samples\n 2.5% 97.5% \n0.574 0.712 \n\n\nOnce again, we have really good internal consistency with our first factor, but a little shaky with our second factor. So, it might be that the first factor is pretty solid, and the other items represent loosely-grouped “other stuff” outside of that factor."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Exploratory Factor Analysis",
    "section": "",
    "text": "Let’s get started with exploratory factor analysis (EFA). This is a method for doing factor analysis when you don’t have an explicit hypothesis about the underlying factor structure (i.e., which variables go with which factors)."
  },
  {
    "objectID": "posts/post-with-code/index.html#data-import-cleaning",
    "href": "posts/post-with-code/index.html#data-import-cleaning",
    "title": "Exploratory Factor Analysis",
    "section": "Data Import & Cleaning",
    "text": "Data Import & Cleaning\nLet’s begin by loading the required packages.\n\n#Load Required Packages\nlibrary(GPArotation)\nlibrary(tidyr)\nlibrary(nFactors)\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(psych)\nlibrary(dplyr)\nlibrary(Rmisc)\nlibrary(coefficientalpha)\nlibrary(lavaan)\nlibrary(plyr)\nlibrary(ggpubr)\nlibrary(mvnormtest)\nlibrary(QuantPsyc)\nlibrary(ltm)\nlibrary(easystats)\nlibrary(performance)\nlibrary(parameters)\nlibrary(knitr)\nlibrary(\"papaja\")\nlibrary(Rmisc)\nlibrary(InteractionPoweR)\nlibrary(magick)\n\nNext, we’ll upload and clean the data (please reach out if, for some reason, you don’t have access to the data). Don’t worry about the data cleaning. It mostly involves list-wise deletion of NAs because factor analysis is really bad with missing data. The questions we’re interested in were only presented to N = 1960 out of 4176 participants, so we have a lot of blank space in the data set corresponding to participants we don’t need to assess. Note that we only remove these NAs for factor analyses; for other analyses, we use other methods."
  },
  {
    "objectID": "posts/post-with-code/index.html#exploratory-data-analysis",
    "href": "posts/post-with-code/index.html#exploratory-data-analysis",
    "title": "Exploratory Factor Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nLet’s briefly take a look at the data. We’ll calculate summary statistics and internal consistency.\n\nsummary.data.frame(SASSEFA) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJewish.people.are.often.very.rude\nJewish.people.tend.to.complain.a.lot\nAll.things.considered.Jewish.people.are.untrustworthy\nJewish.people.can.be.sneaky\nWhen.they.feel.slighted.Jewish.people.will.be.vengeful\nJewish.people.tend.to.be.good.with.money\nJewish.people.tend.to.influence.the.media\nWhen.it.comes.to.education.Jewish.people.tend.to.be.overachievers\nOn.the.whole.Jewish.people.are.loyal.to.Israel\n\n\n\n\n\nMin. :1.00\nMin. :1.000\nMin. :1.000\nMin. :1.000\nMin. :1.000\nMin. :1.000\nMin. :1.000\nMin. :1.000\nMin. :1.000\n\n\n\n1st Qu.:4.00\n1st Qu.:3.000\n1st Qu.:4.000\n1st Qu.:3.000\n1st Qu.:3.000\n1st Qu.:2.000\n1st Qu.:3.000\n1st Qu.:3.000\n1st Qu.:2.000\n\n\n\nMedian :4.00\nMedian :4.000\nMedian :5.000\nMedian :4.000\nMedian :4.000\nMedian :3.000\nMedian :4.000\nMedian :3.000\nMedian :3.000\n\n\n\nMean :4.37\nMean :4.147\nMean :4.724\nMean :4.257\nMean :4.073\nMean :2.805\nMean :3.927\nMean :3.202\nMean :2.894\n\n\n\n3rd Qu.:5.00\n3rd Qu.:5.000\n3rd Qu.:6.000\n3rd Qu.:5.000\n3rd Qu.:5.000\n3rd Qu.:3.000\n3rd Qu.:5.000\n3rd Qu.:4.000\n3rd Qu.:4.000\n\n\n\nMax. :6.00\nMax. :6.000\nMax. :6.000\nMax. :6.000\nMax. :6.000\nMax. :6.000\nMax. :6.000\nMax. :6.000\nMax. :6.000\n\n\n\n\ncronbach.alpha(SASSEFA, CI = TRUE)\n\n\nCronbach's alpha for the 'SASSEFA' data-set\n\nItems: 9\nSample units: 1960\nalpha: 0.832\n\nBootstrap 95% CI based on 1000 samples\n 2.5% 97.5% \n0.817 0.845 \n\n\nOur Cronbach’s alpha is pretty high: 0.832, 95% C:I [0.817, 0.845]. But let’s make a nicer-looking table.\n\n\n\n(#tab:unnamed-chunk-4) Scale of Antisemitic Stereotypes Descriptive Statistics\n\n\nQuestions\nMean\nMedian\nSD\n\n\n\n\nJewish people are often very rude\n4.37\n4.00\n1.08\n\n\nJewish people tend to complain a lot\n4.15\n4.00\n1.14\n\n\nAll things considered, Jewish people are untrustworthy\n4.72\n5.00\n1.08\n\n\nJewish people can be sneaky\n4.26\n4.00\n1.17\n\n\nWhen they feel slighted, Jewish people will be vengeful\n4.07\n4.00\n1.15\n\n\nJewish people tend to influence the media\n3.93\n4.00\n1.16\n\n\nJewish people tend to be good with money\n2.81\n3.00\n1.07\n\n\nWhen it comes to education, Jewish people tend to be overachievers\n3.20\n3.00\n1.10\n\n\nOn the whole, Jewish people are loyal to Israel\n2.89\n3.00\n1.07\n\n\n\n\nNote. Item scores are scored 1-6 (‘strongly disagree’ - strongly agree’)"
  },
  {
    "objectID": "posts/post-with-code/index.html#kaiser-meyer-olkin-test",
    "href": "posts/post-with-code/index.html#kaiser-meyer-olkin-test",
    "title": "Exploratory Factor Analysis",
    "section": "Kaiser-Meyer-Olkin Test",
    "text": "Kaiser-Meyer-Olkin Test\nThe Kaiser-Meyer-Olkin test (KMO) calculates a measure of sampling adequacy (MSA) for each item, and for your items overall. MSA less than 0.50 is really bad.\n\n#Is factor analysis is warranted?\nKMO(SASSEFA) # You should get rid of all variables with MSA &lt; 0.50, which we don't need to do here. \n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = SASSEFA)\nOverall MSA =  0.88\nMSA for each item = \n                                Jewish.people.are.often.very.rude \n                                                             0.89 \n                             Jewish.people.tend.to.complain.a.lot \n                                                             0.91 \n            All.things.considered.Jewish.people.are.untrustworthy \n                                                             0.88 \n                                      Jewish.people.can.be.sneaky \n                                                             0.89 \n           When.they.feel.slighted.Jewish.people.will.be.vengeful \n                                                             0.92 \n                         Jewish.people.tend.to.be.good.with.money \n                                                             0.76 \n                        Jewish.people.tend.to.influence.the.media \n                                                             0.92 \nWhen.it.comes.to.education.Jewish.people.tend.to.be.overachievers \n                                                             0.78 \n                   On.the.whole.Jewish.people.are.loyal.to.Israel \n                                                             0.72 \n\n\nOverall KMO score is 0.88, which is quite good (above 0.90 is ideal, but 0.88 is pretty good)."
  },
  {
    "objectID": "posts/post-with-code/index.html#bartletts-test-of-sphericity",
    "href": "posts/post-with-code/index.html#bartletts-test-of-sphericity",
    "title": "Exploratory Factor Analysis",
    "section": "Bartlett’s Test of Sphericity",
    "text": "Bartlett’s Test of Sphericity\nNext, we do Bartlett’s test of sphericity, which is a linear algebra/matrix-related test we’re not going to get into here. Suffice it to say that Bartlett’s test evaluates whether the variables in our data are sufficiently correlated to proceed with factor analysis.\n\ncortest.bartlett(SASSEFA)\n\nR was not square, finding R from data\n\n\n$chisq\n[1] 7051.592\n\n$p.value\n[1] 0\n\n$df\n[1] 36\n\n\nFun fact, we can use the performance package to conduct both of these tests at once! The output even provides an explanatory example write-up!\n\nperformance::check_factorstructure(SASSEFA)\n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(36) = 7051.59, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.88). The individual KMO scores are: Jewish.people.are.often.very.rude (0.89), Jewish.people.tend.to.complain.a.lot (0.91), All.things.considered.Jewish.people.are.untrustworthy (0.88), Jewish.people.can.be.sneaky (0.89), When.they.feel.slighted.Jewish.people.will.be.vengeful (0.92), Jewish.people.tend.to.be.good.with.money (0.76), Jewish.people.tend.to.influence.the.media (0.92), When.it.comes.to.education.Jewish.people.tend.to.be.overachievers (0.78), On.the.whole.Jewish.people.are.loyal.to.Israel (0.72)."
  },
  {
    "objectID": "posts/post-with-code/index.html#kaiser-criterion-eigenvalues",
    "href": "posts/post-with-code/index.html#kaiser-criterion-eigenvalues",
    "title": "Exploratory Factor Analysis",
    "section": "Kaiser Criterion (Eigenvalues)",
    "text": "Kaiser Criterion (Eigenvalues)\nAccording to Kaiser, if a factor has an Eigenvalue of 1 or higher, that factor should be retained.\n“Eigenvalue” is another linear algebra term. It essentially tells you how data are spread out on a “line,” or in this case, an Eigenvector. Eigenvalues are scalar transformations applied to Eigenvectors, which themselves have magnitudes and directions (this will be important when we talk about rotation in a minute).\n\nev &lt;- eigen(cor(SASSEFA)) # get Eigenvalues\nev$values\n\n[1] 4.0736669 1.5975626 0.7194669 0.5841090 0.4973981 0.4804799 0.4135245\n[8] 0.3182940 0.3154982\n\n\nAccording to the output of the above code, the first two factors out of a potential 9 factors (i.e., one factor per variable, which would defeat the purpose of having factors) have Eigenvalues above one. This would suggest we retain two factors.\nWe can also get Eigenvalues from Scree plots, like the ones in the code chunk below. The second plot overlays parallel analysis, which used actual, simulated, and re-sampled data.\n\nscree(SASSEFA, pc=FALSE)  # Use pc=FALSE for factor analysis. \"PC\" refers to principal components analysis, which is beyond the scope of this tutorial (though it is related).\n\n\n\n\n\n\n\nfa.parallel(SASSEFA, fa=\"fa\") #FA for Factor Analysis.\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n\nIn Scree plots, we’re looking for the “elbow” of the plot; that is, the inflection point after which each factor you add has Eigenvalues below 1. As the output helpfully tells us, this would suggest 2 factors."
  },
  {
    "objectID": "posts/post-with-code/index.html#consensus-method",
    "href": "posts/post-with-code/index.html#consensus-method",
    "title": "Exploratory Factor Analysis",
    "section": "Consensus Method",
    "text": "Consensus Method\nWhile Kaiser is arguable the most common method for determining the number of factors, there’s a neat function called n_factors which compares across multiple criteria for factor inclusion. Basically, it surveys multiple methods of choosing the number of factors you want, then gives you a sort of histogram to tell you how many methods encourage using each possible number of factors.\n\n#Factor analysis is warranted. We must identify the appropriate number of factors. We set n_max at 9 because that's the largest number of factors we could have in this 9-item data set. \nn_factors(SASSEFA, n_max = 9) %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nAccording to this plot, 2 factors seems to be the consensus among different methods. The dashed line represents the cumulative variance in the data explained by each number of factors you could extract. For 2 factors, we’re looking at just above 50% variance explained. Ideally, for factor analysis, we would prefer to see something closer to 70%."
  },
  {
    "objectID": "posts/post-with-code/index.html#rotation",
    "href": "posts/post-with-code/index.html#rotation",
    "title": "Exploratory Factor Analysis",
    "section": "Rotation",
    "text": "Rotation\n“Rotation” has to do with how different the directions of vectors are. In our terms, it refers to how correlated or uncorrelated we think our factors should be.\nIf we assume our factors are entirely uncorrelated (i.e., independent), then we would use an orthogonal rotation to find our factor loadings.\n\nOrthogonal Rotations\n\nVarimax Rotation\nVarimax rotation tries to reduce cross-loading and minimizes smaller loadings. This gives you clear, unambiguous factor structures.\n\n\nQuartimax Rotation\nQuartimax rotation tries to reduce the number of variables you need to comprise each factor. This makes interpretation simpler, but potentially at loss of some complexity.\n\n\nEquamax Rotation\nThis is just a compromise between varimax and quartimax.\nAs an aside, orthogonality in 2D geometry is perpendicularity. So to understand orthogonality, think about two lines separated by a 90-degree angle, like an X- and Y-axis. No matter how far you travel along one axis, you don’t move at all along the other axis. These two axes, also called dimensions are thus independent, or not correlated. As we discussed, our factors are, in theory terms, a dimension. Furthermore, our factors can be expressed mathematically as vectors. Therefore, assuming an orthogonal factor structure, which orthogonal rotations do, would be to assume that one factor is uncorrelated with the other. If we drew the factors/vectors in 2D space, they would be perpendicular.\n\n\n\nOblique Rotation\nAssuming orthogonality can be somewhat restrictive. It’s quite possible, depending on your theoretical question, that your factors will be correlated in some way. To capture this correlation, we can use oblique rotations.\n\nPromax Rotation\nPromax rotation is really good at structuring large data sets. However, it might lead to higher correlations between factors.\n\n\nOblimin Rotation\nThis approach can’t handle large data sets as well as promax; however, it often produces a simple, easier-to-interpret factor structure.\nWhich rotation method you choose depends a lot on your prior knowledge of the phenomena you’re studying. In our case, we are interested in stereotypes, which tend to be correlated. As such, we’ll use an oblique rotation. Past work has used oblimin rotation, so we will use that method here."
  },
  {
    "objectID": "posts/post-with-code/index.html#factoring-method",
    "href": "posts/post-with-code/index.html#factoring-method",
    "title": "Exploratory Factor Analysis",
    "section": "Factoring Method",
    "text": "Factoring Method\nWe also have to specify our factoring method. Type “?fa” into R to find the documentation and a brief rationale for each “fm” (factoring method). The two best suited to our analysis here are fm=“pa,” which gives the principal factor solution, and fm=“ml,” which gives the maximum likelihood factor analysis.\nNote how we include the rotation method and factoring method in the code below\n\n#Oblimin Rotation; principal factor solution\nSASS_efa_pa &lt;- psych::fa(SASSEFA, nfactors = 2, rotate=\"oblimin\", fm=\"pa\") %&gt;% model_parameters(sort = TRUE, threshold = \"max\")\n\nSASS_efa_pa\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable                                                          |  PA1 |  PA2\n-------------------------------------------------------------------------------\nJewish.people.are.often.very.rude                                 | 0.83 |     \nJewish.people.can.be.sneaky                                       | 0.82 |     \nAll.things.considered.Jewish.people.are.untrustworthy             | 0.80 |     \nWhen.they.feel.slighted.Jewish.people.will.be.vengeful            | 0.73 |     \nJewish.people.tend.to.complain.a.lot                              | 0.72 |     \nJewish.people.tend.to.influence.the.media                         | 0.54 |     \nJewish.people.tend.to.be.good.with.money                          |      | 0.70\nWhen.it.comes.to.education.Jewish.people.tend.to.be.overachievers |      | 0.57\nOn.the.whole.Jewish.people.are.loyal.to.Israel                    |      | 0.56\n\nVariable                                                          | Complexity | Uniqueness\n-------------------------------------------------------------------------------------------\nJewish.people.are.often.very.rude                                 |       1.01 |       0.33\nJewish.people.can.be.sneaky                                       |       1.00 |       0.32\nAll.things.considered.Jewish.people.are.untrustworthy             |       1.06 |       0.41\nWhen.they.feel.slighted.Jewish.people.will.be.vengeful            |       1.04 |       0.41\nJewish.people.tend.to.complain.a.lot                              |       1.02 |       0.45\nJewish.people.tend.to.influence.the.media                         |       1.45 |       0.55\nJewish.people.tend.to.be.good.with.money                          |       1.01 |       0.48\nWhen.it.comes.to.education.Jewish.people.tend.to.be.overachievers |       1.01 |       0.66\nOn.the.whole.Jewish.people.are.loyal.to.Israel                    |       1.04 |       0.71\n\nThe 2 latent factors (oblimin rotation) accounted for 51.97% of the total variance of the original data (PA1 = 37.71%, PA2 = 14.26%).\n\n#Oblimin Rotation; maximum likelihood solution\n\nSASS_efa_ML &lt;- psych::fa(SASSEFA, nfactors = 2, rotate=\"oblimin\", fm=\"ml\") %&gt;% model_parameters(sort = TRUE, threshold = \"max\")\n\nSASS_efa_ML\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable                                                          |  ML1 |  ML2\n-------------------------------------------------------------------------------\nJewish.people.are.often.very.rude                                 | 0.83 |     \nJewish.people.can.be.sneaky                                       | 0.82 |     \nAll.things.considered.Jewish.people.are.untrustworthy             | 0.81 |     \nWhen.they.feel.slighted.Jewish.people.will.be.vengeful            | 0.73 |     \nJewish.people.tend.to.complain.a.lot                              | 0.71 |     \nJewish.people.tend.to.influence.the.media                         | 0.54 |     \nJewish.people.tend.to.be.good.with.money                          |      | 0.70\nWhen.it.comes.to.education.Jewish.people.tend.to.be.overachievers |      | 0.58\nOn.the.whole.Jewish.people.are.loyal.to.Israel                    |      | 0.56\n\nVariable                                                          | Complexity | Uniqueness\n-------------------------------------------------------------------------------------------\nJewish.people.are.often.very.rude                                 |       1.01 |       0.33\nJewish.people.can.be.sneaky                                       |       1.00 |       0.32\nAll.things.considered.Jewish.people.are.untrustworthy             |       1.07 |       0.40\nWhen.they.feel.slighted.Jewish.people.will.be.vengeful            |       1.04 |       0.41\nJewish.people.tend.to.complain.a.lot                              |       1.02 |       0.45\nJewish.people.tend.to.influence.the.media                         |       1.43 |       0.56\nJewish.people.tend.to.be.good.with.money                          |       1.01 |       0.49\nWhen.it.comes.to.education.Jewish.people.tend.to.be.overachievers |       1.00 |       0.66\nOn.the.whole.Jewish.people.are.loyal.to.Israel                    |       1.04 |       0.70\n\nThe 2 latent factors (oblimin rotation) accounted for 51.99% of the total variance of the original data (ML1 = 37.72%, ML2 = 14.27%).\n\n\nColumns PA1 and PA2 give you the factor loadings for each factor in the principal factor solution, while columns ML1 and ML2 give factor loadings for each variable in the maximum likelihood solution. The “threshold = MAX” argument hides cross-loadings for each variable on the other factor (i.e., the factor onto which they load less strongly).\nAlthough the two solutions (PA and ML) reveal virtually no difference with out data, it’s good to be familiar with multiple factoring methods in EFA.\nFrom the above output, we can see that 6 of our variables load onto the first factor, which explains roughly 38% of the variance. 3 variables load onto the second factor, which explains just over 14% of the variance. Cumulatively, our factors explain roughly 52% of the variance in our data (which is what we saw in the consensus methods plot above). 52% isn’t super high, but it’s a good start.\nLooking at uniqueness, which refers to the proportion of variance for each variable that the factors explain, we see that the items on factor 2 tend to have higher uniqueness, meaning their variance is less well explained by our factors. The last item is particularly unique, despite it loading best onto factor 2."
  },
  {
    "objectID": "posts/post-with-code/references.html",
    "href": "posts/post-with-code/references.html",
    "title": "Further Reading",
    "section": "",
    "text": "References\nBartlett, M. S. (1938). Methods of estimating mental factors. Nature, 141, 609–610. doi:10.1038/141246a0.\nBartlett, M. S. (1937). The statistical conception of mental factors. British Journal of Psychology, 28, 97–104. doi:10.1111/j.2044-8295.1937.tb00863.x.\nBollen, K. A. (1989). Structural equations with latent variables. New York: John Wiley & Sons.\nBryant, F. B., & Yarnold, P. R. (1995). Principal components analysis and exploratory and confirmatory factor analysis. In L. G. Grimm & P. R. Yarnold (Eds.), Reading and understanding multivariate analysis. Washington, DC: American Psychological Association.\nField, A., Field, Z., & Miles, J. (2012). Discovering statistics using R.\nFiske, S. T. (2018). Stereotype Content: Warmth and Competence Endure. Current Directions in Psychological Science, 27(2), 67-73. https://doi.org/10.1177/0963721417738825 (Original work published 2018)\nFiske, S. T., Cuddy, A. J., & Glick, P. (2007). Universal dimensions of social cognition: warmth and competence. Trends in cognitive sciences, 11(2), 77–83. https://doi.org/10.1016/j.tics.2006.11.005\nHu, L., & Bentler, P. M. (1999). Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives. Structural Equation Modeling: A Multidisciplinary Journal, 6(1), 1–55. https://doi.org/10.1080/10705519909540118\nSee also:\nhttps://www.datacamp.com/doc/r/factor https://rpubs.com/pjmurphy/758265"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Packman’s Factor Analysis Tutorial (504 Blog)",
    "section": "",
    "text": "Further Reading\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\nJames Packman\n\n\n\n\n\n\n\n\n\n\n\n\nConfirmatory Factor Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nJames Packman\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Factor Analysis\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 27, 2025\n\n\nJames Packman\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Factor Analysis\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nJames Packman\n\n\n\n\n\n\nNo matching items"
  }
]